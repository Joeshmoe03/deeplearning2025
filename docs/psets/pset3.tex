\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref,bbm, graphicx}
\usepackage{bm}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{CSCI 1051 Problem Set 3}
\author{} % TODO: Put your name here
\date{\today}

\begin{document}

\maketitle

\subsection*{Submission Instructions}

Please upload your solutions by
\textbf{5pm Friday January 24, 2025.}
\begin{itemize}
\item You are encouraged to discuss ideas
and work with your classmates. However, you
\textbf{must acknowledge} your collaborators
at the top of each solution on which
you collaborated with others 
and you \textbf{must write} your solutions
independently.
\item Your solutions to theory questions must
be written legibly, or typeset in LaTeX or markdown.
If you would like to use LaTeX, you can import the source of this document 
\href{https://www.rtealwitter.com/deeplearning/psets/pset2.tex}{here}
to Overleaf.
\item I recommend that you write your solutions to coding questions in a Jupyter notebook using Google Colab.
\item You should submit your solutions as a \textbf{single PDF} via the assignment on Gradescope.
\end{itemize}

\newpage \section*{Problem 1: Image Embeddings}

In this problem, we will embed images using an autoencoder that you train from scratch.
I recommend that use one of the datasets from the MNIST family.

\subsection*{Part A: Autoencoder Training}

Train a small (three or so layers with activations) autoencoder to produce embeddings in two dimensions via reconstruction loss.
I suggest writing one class for the encoder and one for the decoder.

\subsection*{Part B: Plots}

Take (a subset of) images in your training data, encode them with the encoder portion of the autoencoder and plot them on a scatter plot.

Take a grid of points (about 10 by 10) in the range of points from the prior plot and pass them through your decoder.
Now plot the resulting images on a grid based on their latent dimension.

\subsection*{Part C: Variational Autoencoder}

Train a variational autoencoder with reconstruction loss and variational loss simultaneously.
Remember that the encoder will reproduce two vectors that you will interpret as the mean $\bm{\mu} \in \mathbb{R}^2$ and standard deviation $\bm{\sigma} \in \mathbb{R}_+^2$ of the latent vector.
You can find the latent vector by computing $\mathbf{z} = \bm{\mu} + \bm{\sigma} \bm{\epsilon}$ where $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \bm{I}).$

The KL divergence (aka cross entropy) loss simplifies to something like
\begin{align}
    \sum_{i=1}^2 -\log(\sigma_i) + \sigma_i^2 + \mu_i^2.
\end{align}

\subsection*{Part D: Variational Plots}

Reproduce the same plots that you created above for the variational autoencoder.
What do you notice?

%\input{solutions/solution3_1}

\end{document}